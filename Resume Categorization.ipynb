{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Resume Categorization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project tackles the challenge of **resume categorization** using machine learning and deep learning techniques. Companies often face the daunting task of sifting through numerous resumes for each job opening. This app aims to automate and streamline this process by predicting the job category a given resume belongs to. By using a trained model, the app can quickly suggest the appropriate job category for each resume, saving time and resources for recruiters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Set Environmnt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reading The Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Acquisition:** The project uses the [Resume Dataset](https://www.kaggle.com/datasets/gauravduttakiit/resume-dataset) from Kaggle. This dataset consists of resumes categorized into 25 distinct job fields, providing a solid foundation for training our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumeDataSet = pd.read_csv(\"UpdatedResumeDataSet.csv\")\n",
    "resumeDataSet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Displaying the distinct categories of resume**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Displaying the distinct categories of resume:\\n\\n \")\n",
    "print (resumeDataSet['Category'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Displaying the number of resumes in each category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Displaying the distinct categories of resume and the number of records belonging to each category:\\n\\n\")\n",
    "print (resumeDataSet['Category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Check the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumeDataSet['Category'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumeDataSet['Resume'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Text preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Preprocessing:** Raw resume text is often messy. We apply preprocessing techniques to clean the resume data, including:\n",
    "- Removing URLs, RTs, hashtags, and mentions\n",
    "- Eliminating special characters and non-ASCII characters\n",
    "- Collapsing extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleanResume(txt):\n",
    "    cleanText = re.sub('http\\S+\\s', ' ', txt) # This line removes any URLs from the text\n",
    "    cleanText = re.sub('RT|cc', ' ', cleanText) # This line removes any RTs or cc from the text\n",
    "    cleanText = re.sub('#\\S+\\s', ' ', cleanText) # This line removes any hashtags from the text\n",
    "    cleanText = re.sub('@\\S+', '  ', cleanText) # This line removes any @ from the text\n",
    "    cleanText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', cleanText) # This line removes any punctuations from the text\n",
    "    cleanText = re.sub(r'[^\\x00-\\x7f]', ' ', cleanText) # This line removes any non-ASCII characters from the text\n",
    "    cleanText = re.sub('\\s+', ' ', cleanText) # This line removes any extra whitespaces from the text\n",
    "    return cleanText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumeDataSet['Resume'] = resumeDataSet['Resume'].apply(lambda x: cleanResume(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Check cleaned text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumeDataSet['Resume'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Data Preparation:**\n",
    "- **Category Encoding:** We transform the textual categories into numerical labels using Label Encoding, allowing our machine learning algorithms to work with the data.\n",
    "- **TF-IDF Vectorization:** We convert the cleaned text data into numerical vectors using TF-IDF (Term Frequency-Inverse Document Frequency), which gives more weight to words that are specific to a document in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Category Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit(resumeDataSet['Category'])\n",
    "resumeDataSet['Category'] = le.transform(resumeDataSet['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumeDataSet.Category.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "tfidf.fit(resumeDataSet['Resume'])\n",
    "requredTaxt  = tfidf.transform(resumeDataSet['Resume'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Splitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Splitting:** The dataset is divided into training and testing sets to properly evaluate the performance of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(requredTaxt, resumeDataSet['Category'], test_size=0.2, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Development**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Development:** We build and train multiple models:\n",
    " - **Machine Learning Models:**\n",
    "    *   **K-Nearest Neighbors (KNN):** A simple yet effective classification algorithm based on distance between points.\n",
    "    *   **Support Vector Machine (SVC):** A powerful algorithm that finds an optimal hyperplane to separate classes.\n",
    "    *   **Random Forest:** An ensemble method that combines multiple decision trees for more robust predictions.\n",
    "-   **Deep Learning Model:**\n",
    "    *   **Multilayer Perceptron (MLP):** A neural network model with multiple hidden layers to learn complex patterns from the data.\n",
    "-   **Ensemble Model**\n",
    "    *   **Voting Classifier:** Combines the predictions from the machine learning models to achieve more accurate and robust results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Machine Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that X_train and X_test are dense if they are sparse\n",
    "X_train = X_train.toarray() if hasattr(X_train, 'toarray') else X_train\n",
    "X_test = X_test.toarray() if hasattr(X_test, 'toarray') else X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Train KNeighborsClassifier\n",
    "knn_model = OneVsRestClassifier(KNeighborsClassifier())\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Accuracy for the training set\n",
    "y_train_pred_knn = knn_model.predict(X_train)\n",
    "train_accuracy_knn = accuracy_score(y_train, y_train_pred_knn)\n",
    "print(f\"Training Accuracy: {train_accuracy_knn:.4f}\")\n",
    "\n",
    "# Accuracy for the test set\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "test_accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\" Testing Accuracy: {test_accuracy_knn:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_knn)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_knn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Support Vector Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Train SVC\n",
    "svc_model = OneVsRestClassifier(SVC())\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "# Accuracy for the training set\n",
    "y_train_pred_svc = svc_model.predict(X_train)\n",
    "train_accuracy_svc = accuracy_score(y_train, y_train_pred_svc)\n",
    "print(f\"Training Accuracy: {train_accuracy_svc:.4f}\")\n",
    "\n",
    "# Accuracy for the test set\n",
    "y_pred_svc = svc_model.predict(X_test)\n",
    "test_accuracy_svc = accuracy_score(y_test, y_pred_svc)\n",
    "print(\"\\nSVC Results:\")\n",
    "print(f\"Testing Accuracy: {test_accuracy_svc:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_svc)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_svc)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Train RandomForestClassifier\n",
    "rf_model = OneVsRestClassifier(RandomForestClassifier())\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Accuracy for the training set\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "train_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "print(f\"Training Accuracy: {train_accuracy_rf:.4f}\")\n",
    "\n",
    "# Accuracy for the test set\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "test_accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"\\nRandomForestClassifier Results:\")\n",
    "print(f\"Testing Accuracy: {test_accuracy_rf:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_rf)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_rf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ensemble Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Voting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Create a VotingClassifier with the three models\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('knn', knn_model),\n",
    "    ('svc', svc_model),\n",
    "    ('rf', rf_model)\n",
    "], voting='hard')\n",
    "\n",
    "# Train the ensemble model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with the ensemble model\n",
    "y_pred_ensemble = ensemble_model.predict(X_test)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "print(f\"Ensemble Model Accuracy: {ensemble_accuracy:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_ensemble)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_ensemble)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "# Define the MLP model\n",
    "model = keras.Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer\n",
    "    Dense(64, activation='relu'),  # Hidden layer\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')  # Output layer (number of classes)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',  # Use sparse_categorical_crossentropy for integer labels\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "# Ensure X_train is dense if it is sparse\n",
    "X_train_dense = X_train.toarray() if hasattr(X_train, 'toarray') else X_train\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_dense, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Retrieve training and validation accuracy from history\n",
    "train_accuracy = history.history['accuracy'][-1]  # Last epoch's training accuracy\n",
    "val_accuracy = history.history['val_accuracy'][-1]  # Last epoch's validation accuracy\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_mlp = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Deep Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **RNN with GD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(resumeDataSet['Resume'])\n",
    "X_sequences = tokenizer.texts_to_sequences(resumeDataSet['Resume'])\n",
    "X_padded = pad_sequences(X_sequences, maxlen=500)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, resumeDataSet['Category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the RNN model\n",
    "rnn_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=500),\n",
    "    SimpleRNN(64, return_sequences=False),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "rnn_model.compile(optimizer=sgd_optimizer, \n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_rnn = rnn_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Retrieve training and validation accuracy from history\n",
    "train_accuracy_rnn = history_rnn.history['accuracy'][-1]\n",
    "val_accuracy_rnn = history_rnn.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accuracy_rnn:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy_rnn:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss_rnn, test_accuracy_rnn = rnn_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss_rnn:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_rnn:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rnn = np.argmax(rnn_model.predict(X_test), axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "rnn_model.compile(optimizer=sgd_optimizer, \n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_rnn = rnn_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Retrieve training and validation accuracy from history\n",
    "train_accuracy_rnn = history_rnn.history['accuracy'][-1]\n",
    "val_accuracy_rnn = history_rnn.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accuracy_rnn:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy_rnn:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss_rnn, test_accuracy_rnn = rnn_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss_rnn:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_rnn:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rnn = np.argmax(rnn_model.predict(X_test), axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_rnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **RNN with Adam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(resumeDataSet['Resume'])\n",
    "sequences = tokenizer.texts_to_sequences(resumeDataSet['Resume'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "# Encode the labels\n",
    "le = LabelEncoder()\n",
    "le.fit(resumeDataSet['Category'])\n",
    "encoded_labels = le.transform(resumeDataSet['Category'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the RNN model\n",
    "rnn_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=200),\n",
    "    SimpleRNN(64, return_sequences=False),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "rnn_model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_rnn = rnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Retrieve training and validation accuracy from history\n",
    "train_accuracy_rnn = history_rnn.history['accuracy'][-1]\n",
    "val_accuracy_rnn = history_rnn.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accuracy_rnn:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy_rnn:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss_rnn, test_accuracy_rnn = rnn_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss_rnn:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_rnn:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rnn = np.argmax(rnn_model.predict(X_test), axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_rnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(resumeDataSet['Resume'])\n",
    "sequences = tokenizer.texts_to_sequences(resumeDataSet['Resume'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "# Encode the labels\n",
    "le = LabelEncoder()\n",
    "le.fit(resumeDataSet['Category'])\n",
    "encoded_labels = le.transform(resumeDataSet['Category'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the LSTM model\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=200),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(optimizer='adam',\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_lstm = lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Retrieve training and validation accuracy from history\n",
    "train_accuracy_lstm = history_lstm.history['accuracy'][-1]\n",
    "val_accuracy_lstm = history_lstm.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accuracy_lstm:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy_lstm:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss_lstm, test_accuracy_lstm = lstm_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss_lstm:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_lstm:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lstm = np.argmax(lstm_model.predict(X_test), axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Ensure padded_sequences is defined\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(resumeDataSet['Resume'])\n",
    "sequences = tokenizer.texts_to_sequences(resumeDataSet['Resume'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "# Ensure y_train is a NumPy array\n",
    "if not isinstance(y_train, np.ndarray):\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "# Calculate the number of unique classes\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# Define the LSTM model\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=200),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(optimizer='adam',\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Ensure resumeDataSet['Category'] is a NumPy array\n",
    "if not isinstance(resumeDataSet['Category'], np.ndarray):\n",
    "    encoded_labels = np.array(resumeDataSet['Category'])\n",
    "\n",
    "# Train the model\n",
    "history_lstm = lstm_model.fit(padded_sequences, encoded_labels, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Retrieve training and validation accuracy from history\n",
    "train_accuracy_lstm = history_lstm.history['accuracy'][-1]\n",
    "val_accuracy_lstm = history_lstm.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accuracy_lstm:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy_lstm:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "# Use the test indices from the train_test_split function\n",
    "# Get the original resume text corresponding to the test set indices\n",
    "test_resume_texts = resumeDataSet['Resume'].iloc[:len(X_test)].tolist()\n",
    "\n",
    "# Now use this list of texts for tokenization\n",
    "test_sequences = tokenizer.texts_to_sequences(test_resume_texts)\n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "loss_lstm, test_accuracy_lstm = lstm_model.evaluate(padded_test_sequences, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss_lstm:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_lstm:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lstm = np.argmax(lstm_model.predict(padded_test_sequences), axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create an indexed Series for y_test\n",
    "y_test1 = pd.Series(y_test, index=range(len(y_test)))\n",
    "\n",
    "# Display the indexed Series\n",
    "print(y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Ensure padded_sequences is defined\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(resumeDataSet['Resume'])\n",
    "sequences = tokenizer.texts_to_sequences(resumeDataSet['Resume'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "# Define the LSTM model\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=200),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(optimizer='adam',\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "# Ensure labels are converted to a NumPy array\n",
    "category_labels = np.array(resumeDataSet['Category'])\n",
    "\n",
    "# Train the model\n",
    "history_lstm = lstm_model.fit(padded_sequences, category_labels, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Retrieve training and validation accuracy from history\n",
    "train_accuracy_lstm = history_lstm.history['accuracy'][-1]\n",
    "val_accuracy_lstm = history_lstm.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accuracy_lstm:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy_lstm:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "# Get the original resume text corresponding to the test set indices\n",
    "test_resume_texts = resumeDataSet['Resume'].iloc[:len(y_test1)].tolist()\n",
    "  \n",
    "\n",
    "# Now use this list of texts for tokenization\n",
    "test_sequences = tokenizer.texts_to_sequences(test_resume_texts)  \n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "loss_lstm, test_accuracy_lstm = lstm_model.evaluate(padded_test_sequences, y_test)\n",
    "print(f\"Test Loss: {loss_lstm:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_lstm:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lstm = np.argmax(lstm_model.predict(padded_test_sequences), axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LSTM with GD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Define the LSTM model\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=200),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with SGD optimizer\n",
    "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "lstm_model.compile(optimizer=sgd_optimizer,\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_lstm_sgd = lstm_model.fit(padded_sequences, resumeDataSet['Category'], epochs=70, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Retrieve training and validation accuracy from history\n",
    "train_accuracy_lstm_sgd = history_lstm_sgd.history['accuracy'][-1]\n",
    "val_accuracy_lstm_sgd = history_lstm_sgd.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"Final Training Accuracy with SGD: {train_accuracy_lstm_sgd:.4f}\")\n",
    "print(f\"Final Validation Accuracy with SGD: {val_accuracy_lstm_sgd:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss_lstm_sgd, test_accuracy_lstm_sgd = lstm_model.evaluate(padded_test_sequences, y_test)\n",
    "print(f\"Test Loss with SGD: {loss_lstm_sgd:.4f}\")\n",
    "print(f\"Test Accuracy with SGD: {test_accuracy_lstm_sgd:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lstm_sgd = np.argmax(lstm_model.predict(padded_test_sequences), axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_lstm_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BI-LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, GlobalMaxPool1D, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(resumeDataSet['Resume'])\n",
    "sequences = tokenizer.texts_to_sequences(resumeDataSet['Resume'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "# Encode the labels\n",
    "le = LabelEncoder()\n",
    "le.fit(resumeDataSet['Category'])\n",
    "encoded_labels = le.transform(resumeDataSet['Category'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the BI-LSTM model\n",
    "bi_lstm_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=200),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "bi_lstm_model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_bi_lstm = bi_lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Retrieve training and validation accuracy from history\n",
    "train_accuracy_bi_lstm = history_bi_lstm.history['accuracy'][-1]\n",
    "val_accuracy_bi_lstm = history_bi_lstm.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accuracy_bi_lstm:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy_bi_lstm:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss_bi_lstm, test_accuracy_bi_lstm = bi_lstm_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss_bi_lstm:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_bi_lstm:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_bi_lstm = np.argmax(bi_lstm_model.predict(X_test), axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_bi_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(resumeDataSet['Resume'])\n",
    "sequences = tokenizer.texts_to_sequences(resumeDataSet['Resume'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "# Define the BI-LSTM model\n",
    "bi_lstm_model = keras.Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=200),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "bi_lstm_model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_bi_lstm = bi_lstm_model.fit(padded_sequences, resumeDataSet['Category'], epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Retrieve training and validation accuracy from history\n",
    "train_accuracy_bi_lstm = history_bi_lstm.history['accuracy'][-1]\n",
    "val_accuracy_bi_lstm = history_bi_lstm.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accuracy_bi_lstm:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy_bi_lstm:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "# Get the original resume text corresponding to the test set indices\n",
    "test_resume_texts = resumeDataSet['Resume'].iloc[y_test.index].tolist()  \n",
    "\n",
    "# Now use this list of texts for tokenization\n",
    "test_sequences = tokenizer.texts_to_sequences(test_resume_texts)  \n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "loss_bi_lstm, test_accuracy_bi_lstm = bi_lstm_model.evaluate(padded_test_sequences, y_test)\n",
    "print(f\"Test Loss: {loss_bi_lstm:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_bi_lstm:.4f}\")\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "y_pred_bi_lstm = np.argmax(bi_lstm_model.predict(padded_test_sequences), axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_bi_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BI-LSTM with GD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Define the BI-LSTM model\n",
    "bi_lstm_model_sgd = keras.Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=200),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with SGD optimizer\n",
    "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "bi_lstm_model_sgd.compile(optimizer=sgd_optimizer,\n",
    "                          loss='sparse_categorical_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_bi_lstm_sgd = bi_lstm_model_sgd.fit(padded_sequences, resumeDataSet['Category'], epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Retrieve training and validation accuracy from history\n",
    "train_accuracy_bi_lstm_sgd = history_bi_lstm_sgd.history['accuracy'][-1]\n",
    "val_accuracy_bi_lstm_sgd = history_bi_lstm_sgd.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"Final Training Accuracy with SGD: {train_accuracy_bi_lstm_sgd:.4f}\")\n",
    "print(f\"Final Validation Accuracy with SGD: {val_accuracy_bi_lstm_sgd:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss_bi_lstm_sgd, test_accuracy_bi_lstm_sgd = bi_lstm_model_sgd.evaluate(padded_test_sequences, y_test)\n",
    "print(f\"Test Loss with SGD: {loss_bi_lstm_sgd:.4f}\")\n",
    "print(f\"Test Accuracy with SGD: {test_accuracy_bi_lstm_sgd:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_bi_lstm_sgd = np.argmax(bi_lstm_model_sgd.predict(padded_test_sequences), axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_bi_lstm_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def _init_(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def _len_(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def _getitem_(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer)\n",
    "test_dataset = TextDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Define the model, optimizer, and loss function\n",
    "model = BertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels= 25)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for d in data_loader:\n",
    "        input_ids = d['input_ids'].to(device)\n",
    "        attention_mask = d['attention_mask'].to(device)\n",
    "        labels = d['labels'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d['input_ids'].to(device)\n",
    "            attention_mask = d['attention_mask'].to(device)\n",
    "            labels = d['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch( model, train_loader, loss_fn, optimizer, device, None, len(train_dataset))\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    test_acc, test_loss = eval_model(model, test_loader, loss_fn, device, len(test_dataset))\n",
    "\n",
    "    print(f'Test loss {test_loss} accuracy {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Deployment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Deployment:** The trained models, along with the TF-IDF vectorizer and label encoder, are serialized (pickled) for use in the Streamlit application. This step makes the models reusable without retraining each time, **and it is where Streamlit comes into play**. This app was built using Streamlit, a python library to build interactive and sharable web applications. Streamlit made it easier to build this application by handling the user interface, input/output and allowing to display the predictions of different models to be accessible through a web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(tfidf,open('tfidf.pkl','wb'))\n",
    "pickle.dump(knn_model, open('knn.pkl', 'wb'))\n",
    "pickle.dump(svc_model, open('svc.pkl', 'wb'))\n",
    "pickle.dump(rf_model, open('rf.pkl', 'wb'))\n",
    "pickle.dump(model, open('mlp.pkl', 'wb'))\n",
    "pickle.dump(ensemble_model, open('ensemble.pkl', 'wb'))\n",
    "pickle.dump(le, open(\"encoder.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction:** The app provides predictions from all the trained models, allowing for an extensive comparison of their results. The user receives a result from each model, including KNN, SVC, Random Forest, MLP, and the ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the category of a resume and print results for each model\n",
    "def pred(input_resume):\n",
    "    # Preprocess the input text (e.g., cleaning, etc.)\n",
    "    cleaned_text = cleanResume(input_resume)\n",
    "\n",
    "    # Vectorize the cleaned text using the same TF-IDF vectorizer used during training\n",
    "    vectorized_text = tfidf.transform([cleaned_text])\n",
    "\n",
    "    # Convert sparse matrix to dense\n",
    "    vectorized_text = vectorized_text.toarray()\n",
    "\n",
    "    # Prediction\n",
    "    predicted_category_knn = knn_model.predict(vectorized_text)\n",
    "    predicted_category_svc = svc_model.predict(vectorized_text) \n",
    "    predicted_category_rf = rf_model.predict(vectorized_text)\n",
    "    predicted_category_mlp = np.argmax(model.predict(vectorized_text), axis=1)\n",
    "    predicted_category_ensemble = ensemble_model.predict(vectorized_text)\n",
    "\n",
    "    # Get name of predicted category for each model\n",
    "    category_knn = le.inverse_transform(predicted_category_knn)[0]\n",
    "    category_svc = le.inverse_transform(predicted_category_svc)[0]\n",
    "    category_rf = le.inverse_transform(predicted_category_rf)[0]\n",
    "    category_mlp = le.inverse_transform(predicted_category_mlp)[0]\n",
    "    category_ensemble = le.inverse_transform(predicted_category_ensemble)[0]\n",
    "\n",
    "    # Print results for each model\n",
    "    print(f\"KNN Model Prediction: {category_knn}\")\n",
    "    print(f\"SVC Model Prediction: {category_svc}\")\n",
    "    print(f\"Random Forest Model Prediction: {category_rf}\")\n",
    "    print(f\"MLP Model Prediction: {category_mlp}\")\n",
    "    print(f\"Ensemble Model Prediction: {category_ensemble}\")\n",
    "\n",
    "    # Return the category name predicted by the first model (as an example)\n",
    "    return category_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myresume = \"\"\"Name: Sarah Johnson\n",
    "Contact Information:\n",
    "\n",
    "Phone: +1 555-123-4567\n",
    "Email: sarah.johnson@email.com\n",
    "LinkedIn: linkedin.com/in/sarahjohnson\n",
    "Address: New York, NY\n",
    "Professional Summary\n",
    "A results-driven HR professional with over 6 years of experience in talent acquisition, employee engagement, and HR operations. Skilled in building strong teams, fostering positive work environments, and implementing HR strategies that align with organizational goals. Proficient in HR software and data-driven decision-making to improve workforce management.\n",
    "\n",
    "Key Skills\n",
    "Talent Acquisition and Recruitment\n",
    "Employee Onboarding and Training\n",
    "Performance Management\n",
    "HR Policies and Compliance\n",
    "Employee Relations and Engagement\n",
    "Compensation and Benefits Administration\n",
    "HR Analytics and Reporting\n",
    "HR Software: SAP, Workday, BambooHR\n",
    "Strong Interpersonal and Communication Skills\n",
    "Professional Experience\n",
    "HR Manager\n",
    "BrightPath Solutions | January 2020 – Present\n",
    "\n",
    "Led end-to-end recruitment processes, successfully hiring over 50 candidates annually across various roles.\n",
    "Designed and implemented onboarding programs, reducing new hire turnover by 20%.\n",
    "Developed performance management frameworks, increasing employee productivity by 15%.\n",
    "Conducted employee satisfaction surveys and implemented strategies to enhance engagement.\n",
    "Ensured compliance with labor laws and company policies, minimizing legal risks.\n",
    "HR Generalist\n",
    "Global Reach Inc. | June 2016 – December 2019\n",
    "\n",
    "Supported HR operations, including recruitment, payroll, and benefits administration.\n",
    "Assisted in developing HR policies and communicated updates to employees.\n",
    "Resolved employee grievances, fostering a collaborative workplace.\n",
    "Analyzed HR metrics to identify trends and presented actionable insights to management.\n",
    "HR Coordinator\n",
    "TalentFirst Consulting | March 2014 – May 2016\n",
    "\n",
    "Scheduled interviews and coordinated recruitment activities.\n",
    "Maintained employee records and ensured accuracy in HR databases.\n",
    "Assisted in planning company events and training sessions.\n",
    "Education\n",
    "Bachelor’s Degree in Human Resource Management\n",
    "University of California, Berkeley | 2013\n",
    "\n",
    "Certifications\n",
    "\n",
    "Certified Professional in Human Resources (PHR)\n",
    "SHRM Certified Professional (SHRM-CP)\n",
    "Advanced HR Analytics (Coursera)\n",
    "Achievements\n",
    "Reduced time-to-hire by 30% through process optimization.\n",
    "Increased employee retention by 25% by implementing a mentorship program.\n",
    "Spearheaded diversity and inclusion initiatives, leading to a 40% increase in diverse hires.\n",
    "Languages\n",
    "English (Fluent)\n",
    "Spanish (Intermediate)\n",
    "\"\"\"\n",
    "\n",
    "pred(myresume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myresume = \"\"\"I am a Business Analyst specializing in developing dashboards, \n",
    "reports, and data models to drive performance insights. \n",
    "Proficient in Python, R, SQL, Excel, and Power BI, I excel in data \n",
    "analysis, advanced analytics, and automation of data processes. \n",
    "Skilled in statistical analysis and data visualization, I derive \n",
    "insights for data-driven decisions. Experienced in designing and \n",
    "optimizing data warehouse solutions, managing ETL processes, \n",
    "and ensuring data integrity and security. Additionally, I hold a \n",
    "CCNA certification from Cisco, showcasing my knowledge in \n",
    "networking.\n",
    "\"\"\"\n",
    "\n",
    "pred(myresume)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
